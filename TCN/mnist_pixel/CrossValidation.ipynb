{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from TCN.mnist_pixel.model import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customOneHotEncoder(data):\n",
    "    dataAdjust = data.ljust(200,'0')[:200] # padding if not of length and adjusting the data lenght to get a 200x39 input matrix\n",
    "    # define universe of possible input values\n",
    "    alphabet = '0123456789abcdefghijklmnopqrstuvwxyz,._'\n",
    "    # define a mapping of chars to integers\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    # integer encode input data\n",
    "    integer_encoded = [char_to_int[char] for char in dataAdjust]\n",
    "    #print(integer_encoded)\n",
    "    # one hot encode\n",
    "    onehot_encoded = list()\n",
    "    for i, value in enumerate(integer_encoded):\n",
    "        letter = [0 for _ in range(len(alphabet))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    #print(onehot_encoded) # the real encoding\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocessing(df):\n",
    "    \n",
    "    #prepare the imput data\n",
    "    xString = df.iloc[:,:41].to_string(header=False, index=False, index_names = False).split('\\n')\n",
    "    xList = [','.join(ele.split()) for ele in xString] # gives comma separated strings for each row of DataFrame\n",
    "    xData = []\n",
    "    for string in xList:\n",
    "        stringLower = string.lower()\n",
    "        oneHot = customOneHotEncoder(stringLower)\n",
    "        xData.append(oneHot)\n",
    "    xMid = np.array(xData)\n",
    "    xArray = xMid.transpose(0,2,1) # convert xMid's dim (size, 200, 39) to (size, 39, 200)\n",
    "    \n",
    "    #prepare the label data\n",
    "    df.iloc[:, 41] = np.where(df.iloc[:, 41]=='normal', 0, 1) # replacing normals with 0 and anything else with 1\n",
    "    Ydf = df.iloc[:, 41]\n",
    "    #labelName = Ydf.unique().tolist().sort() # sorted 38 label names\n",
    "    #yArray = Ydf.str.get_dummies().to_numpy() # ndarray of shape(rows/lines, 38)\n",
    "    yArray = Ydf.to_numpy()\n",
    "    \n",
    "    assert xArray.shape[0] == yArray.shape[0], 'unequal input and label sample size'\n",
    "    \n",
    "    \n",
    "    return xArray, yArray # return processed array of input and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def datasetTorch(x, y):\n",
    "    xTorch = torch.tensor(x)\n",
    "    yTorch = torch.tensor(y)\n",
    "    Dataset = torch.utils.data.TensorDataset(xTorch, yTorch)\n",
    "    return Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(dataLoader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1_score = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataLoader: # just 1 batch\n",
    "            model.eval()\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            #total+=target.size[0]\n",
    "            #correct+=(predicted == target).sum().item()\n",
    "            report = classification_report(target, predicted)\n",
    "            print(report)\n",
    "            '''precision += report['macro avg']['precision']\n",
    "            recall += report['macro avg']['recall']\n",
    "            f1_score += report['macro avg']['f1_score']\n",
    "            accuracy += report['accuracy']'''\n",
    "    #print(\"Precision: {}, Recall: {}, F1-Score: {}, Accuracy: {}, AccuracyCust: {}\".format(precision, recall, f1_score, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ep,dataGeneratorTrain):\n",
    "    global steps\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(dataGeneratorTrain):\n",
    "        # print('data Shape: {} target shape: {} data type: {}'.format(data.shape, target.shape, type(data)))\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(-1, input_channels, seq_length)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #print('data Shape: {} target shape: {} data type: {}'.format(data.shape, target.shape, type(data)))\n",
    "        #print(target)\n",
    "        optimizer.zero_grad()\n",
    "        #print(data[0])\n",
    "        data = data.type(torch.FloatTensor)\n",
    "        output = model(data)\n",
    "        #print(output.shape)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        #loss1 = torch.nn.CrossEntropyLoss()\n",
    "        loss = F.nll_loss(output, target) # negative log likelihood\n",
    "        #loss = loss1(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        steps += seq_length\n",
    "        if batch_idx > 0 and batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tSteps: {}'.format(\n",
    "                ep, batch_idx * batch_size, len(dataGeneratorTrain.dataset),\n",
    "                100. * batch_idx / len(dataGeneratorTrain), train_loss.item()/100, steps))\n",
    "            train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataGeneratorTest):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataGeneratorTest:\n",
    "            model.eval()\n",
    "            #data = data.view(-1, input_channels, seq_length)\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = model(data)\n",
    "            #loss1 = torch.nn.CrossEntropyLoss()\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            #test_loss += loss1(output, target).item()\n",
    "            #print(output.data.max(1, keepdim=True)[1])\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(dataGeneratorTest.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(dataGeneratorTest.dataset),\n",
    "            100. * correct / len(dataGeneratorTest.dataset)))\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_classes = 2\n",
    "input_channels = 39\n",
    "seq_length = int(200)\n",
    "epochs = 10\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_sizes = [32] * 6 #hidden nodes times levels \n",
    "kernel_size = 5\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=0.05)\n",
    "\n",
    "\n",
    "lr = 1e-5\n",
    "optimizer = getattr(optim, 'Adam')(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rysul/opt/anaconda3/envs/torchEnv/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "/Users/rysul/opt/anaconda3/envs/torchEnv/lib/python3.8/site-packages/pandas/core/indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/22581 (28%)]\tLoss: 0.699171\tSteps: 20200\n",
      "Train Epoch: 1 [12800/22581 (57%)]\tLoss: 0.690396\tSteps: 40200\n",
      "Train Epoch: 1 [19200/22581 (85%)]\tLoss: 0.689837\tSteps: 60200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-92ceded13070>:10: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "/Users/rysul/opt/anaconda3/envs/torchEnv/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6907, Accuracy: 3398/6452 (53%)\n",
      "\n",
      "Train Epoch: 2 [6400/22581 (28%)]\tLoss: 0.696635\tSteps: 90800\n",
      "Train Epoch: 2 [12800/22581 (57%)]\tLoss: 0.688392\tSteps: 110800\n",
      "Train Epoch: 2 [19200/22581 (85%)]\tLoss: 0.689375\tSteps: 130800\n",
      "\n",
      "Test set: Average loss: 0.6880, Accuracy: 3398/6452 (53%)\n",
      "\n",
      "Train Epoch: 3 [6400/22581 (28%)]\tLoss: 0.694167\tSteps: 161400\n",
      "Train Epoch: 3 [12800/22581 (57%)]\tLoss: 0.684070\tSteps: 181400\n",
      "Train Epoch: 3 [19200/22581 (85%)]\tLoss: 0.683788\tSteps: 201400\n",
      "\n",
      "Test set: Average loss: 0.6799, Accuracy: 3398/6452 (53%)\n",
      "\n",
      "Train Epoch: 4 [6400/22581 (28%)]\tLoss: 0.685246\tSteps: 232000\n",
      "Train Epoch: 4 [12800/22581 (57%)]\tLoss: 0.669545\tSteps: 252000\n",
      "Train Epoch: 4 [19200/22581 (85%)]\tLoss: 0.663298\tSteps: 272000\n",
      "\n",
      "Test set: Average loss: 0.6455, Accuracy: 4352/6452 (67%)\n",
      "\n",
      "Train Epoch: 5 [6400/22581 (28%)]\tLoss: 0.643048\tSteps: 302600\n",
      "Train Epoch: 5 [12800/22581 (57%)]\tLoss: 0.601871\tSteps: 322600\n",
      "Train Epoch: 5 [19200/22581 (85%)]\tLoss: 0.543308\tSteps: 342600\n",
      "\n",
      "Test set: Average loss: 0.4460, Accuracy: 5883/6452 (91%)\n",
      "\n",
      "Train Epoch: 6 [6400/22581 (28%)]\tLoss: 0.398955\tSteps: 373200\n",
      "Train Epoch: 6 [12800/22581 (57%)]\tLoss: 0.288804\tSteps: 393200\n",
      "Train Epoch: 6 [19200/22581 (85%)]\tLoss: 0.231002\tSteps: 413200\n",
      "\n",
      "Test set: Average loss: 0.1924, Accuracy: 5954/6452 (92%)\n",
      "\n",
      "Train Epoch: 7 [6400/22581 (28%)]\tLoss: 0.197986\tSteps: 443800\n",
      "Train Epoch: 7 [12800/22581 (57%)]\tLoss: 0.182067\tSteps: 463800\n",
      "Train Epoch: 7 [19200/22581 (85%)]\tLoss: 0.174099\tSteps: 483800\n",
      "\n",
      "Test set: Average loss: 0.1514, Accuracy: 6189/6452 (96%)\n",
      "\n",
      "Train Epoch: 8 [6400/22581 (28%)]\tLoss: 0.159575\tSteps: 514400\n",
      "Train Epoch: 8 [12800/22581 (57%)]\tLoss: 0.145664\tSteps: 534400\n",
      "Train Epoch: 8 [19200/22581 (85%)]\tLoss: 0.144815\tSteps: 554400\n",
      "\n",
      "Test set: Average loss: 0.1307, Accuracy: 6205/6452 (96%)\n",
      "\n",
      "Train Epoch: 9 [6400/22581 (28%)]\tLoss: 0.137892\tSteps: 585000\n",
      "Train Epoch: 9 [12800/22581 (57%)]\tLoss: 0.137325\tSteps: 605000\n",
      "Train Epoch: 9 [19200/22581 (85%)]\tLoss: 0.129104\tSteps: 625000\n",
      "\n",
      "Test set: Average loss: 0.1173, Accuracy: 6220/6452 (96%)\n",
      "\n",
      "Train Epoch: 10 [6400/22581 (28%)]\tLoss: 0.130003\tSteps: 655600\n",
      "Train Epoch: 10 [12800/22581 (57%)]\tLoss: 0.116483\tSteps: 675600\n",
      "Train Epoch: 10 [19200/22581 (85%)]\tLoss: 0.123299\tSteps: 695600\n",
      "\n",
      "Test set: Average loss: 0.1069, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 1 [6400/22581 (28%)]\tLoss: 0.124700\tSteps: 726200\n",
      "Train Epoch: 1 [12800/22581 (57%)]\tLoss: 0.116384\tSteps: 746200\n",
      "Train Epoch: 1 [19200/22581 (85%)]\tLoss: 0.119793\tSteps: 766200\n",
      "\n",
      "Test set: Average loss: 0.1060, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 2 [6400/22581 (28%)]\tLoss: 0.119135\tSteps: 796800\n",
      "Train Epoch: 2 [12800/22581 (57%)]\tLoss: 0.118365\tSteps: 816800\n",
      "Train Epoch: 2 [19200/22581 (85%)]\tLoss: 0.119918\tSteps: 836800\n",
      "\n",
      "Test set: Average loss: 0.1051, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 3 [6400/22581 (28%)]\tLoss: 0.120116\tSteps: 867400\n",
      "Train Epoch: 3 [12800/22581 (57%)]\tLoss: 0.116873\tSteps: 887400\n",
      "Train Epoch: 3 [19200/22581 (85%)]\tLoss: 0.117337\tSteps: 907400\n",
      "\n",
      "Test set: Average loss: 0.1043, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 4 [6400/22581 (28%)]\tLoss: 0.121366\tSteps: 938000\n",
      "Train Epoch: 4 [12800/22581 (57%)]\tLoss: 0.109370\tSteps: 958000\n",
      "Train Epoch: 4 [19200/22581 (85%)]\tLoss: 0.121553\tSteps: 978000\n",
      "\n",
      "Test set: Average loss: 0.1034, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 5 [6400/22581 (28%)]\tLoss: 0.116719\tSteps: 1008600\n",
      "Train Epoch: 5 [12800/22581 (57%)]\tLoss: 0.117817\tSteps: 1028600\n",
      "Train Epoch: 5 [19200/22581 (85%)]\tLoss: 0.111949\tSteps: 1048600\n",
      "\n",
      "Test set: Average loss: 0.1025, Accuracy: 6228/6452 (97%)\n",
      "\n",
      "Train Epoch: 6 [6400/22581 (28%)]\tLoss: 0.111472\tSteps: 1079200\n",
      "Train Epoch: 6 [12800/22581 (57%)]\tLoss: 0.124857\tSteps: 1099200\n",
      "Train Epoch: 6 [19200/22581 (85%)]\tLoss: 0.111234\tSteps: 1119200\n",
      "\n",
      "Test set: Average loss: 0.1017, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 7 [6400/22581 (28%)]\tLoss: 0.119343\tSteps: 1149800\n",
      "Train Epoch: 7 [12800/22581 (57%)]\tLoss: 0.107800\tSteps: 1169800\n",
      "Train Epoch: 7 [19200/22581 (85%)]\tLoss: 0.110655\tSteps: 1189800\n",
      "\n",
      "Test set: Average loss: 0.1009, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 8 [6400/22581 (28%)]\tLoss: 0.107382\tSteps: 1220400\n",
      "Train Epoch: 8 [12800/22581 (57%)]\tLoss: 0.107594\tSteps: 1240400\n",
      "Train Epoch: 8 [19200/22581 (85%)]\tLoss: 0.115718\tSteps: 1260400\n",
      "\n",
      "Test set: Average loss: 0.1002, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 9 [6400/22581 (28%)]\tLoss: 0.112826\tSteps: 1291000\n",
      "Train Epoch: 9 [12800/22581 (57%)]\tLoss: 0.106012\tSteps: 1311000\n",
      "Train Epoch: 9 [19200/22581 (85%)]\tLoss: 0.124481\tSteps: 1331000\n",
      "\n",
      "Test set: Average loss: 0.0994, Accuracy: 6231/6452 (97%)\n",
      "\n",
      "Train Epoch: 10 [6400/22581 (28%)]\tLoss: 0.117397\tSteps: 1361600\n",
      "Train Epoch: 10 [12800/22581 (57%)]\tLoss: 0.111408\tSteps: 1381600\n",
      "Train Epoch: 10 [19200/22581 (85%)]\tLoss: 0.105701\tSteps: 1401600\n",
      "\n",
      "Test set: Average loss: 0.0986, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 1 [6400/22581 (28%)]\tLoss: 0.106512\tSteps: 1432200\n",
      "Train Epoch: 1 [12800/22581 (57%)]\tLoss: 0.112156\tSteps: 1452200\n",
      "Train Epoch: 1 [19200/22581 (85%)]\tLoss: 0.110587\tSteps: 1472200\n",
      "\n",
      "Test set: Average loss: 0.0985, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 2 [6400/22581 (28%)]\tLoss: 0.105719\tSteps: 1502800\n",
      "Train Epoch: 2 [12800/22581 (57%)]\tLoss: 0.115158\tSteps: 1522800\n",
      "Train Epoch: 2 [19200/22581 (85%)]\tLoss: 0.110859\tSteps: 1542800\n",
      "\n",
      "Test set: Average loss: 0.0985, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 3 [6400/22581 (28%)]\tLoss: 0.116265\tSteps: 1573400\n",
      "Train Epoch: 3 [12800/22581 (57%)]\tLoss: 0.110859\tSteps: 1593400\n",
      "Train Epoch: 3 [19200/22581 (85%)]\tLoss: 0.102742\tSteps: 1613400\n",
      "\n",
      "Test set: Average loss: 0.0984, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 4 [6400/22581 (28%)]\tLoss: 0.110856\tSteps: 1644000\n",
      "Train Epoch: 4 [12800/22581 (57%)]\tLoss: 0.095183\tSteps: 1664000\n",
      "Train Epoch: 4 [19200/22581 (85%)]\tLoss: 0.108578\tSteps: 1684000\n",
      "\n",
      "Test set: Average loss: 0.0983, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 5 [6400/22581 (28%)]\tLoss: 0.109580\tSteps: 1714600\n",
      "Train Epoch: 5 [12800/22581 (57%)]\tLoss: 0.112169\tSteps: 1734600\n",
      "Train Epoch: 5 [19200/22581 (85%)]\tLoss: 0.103892\tSteps: 1754600\n",
      "\n",
      "Test set: Average loss: 0.0983, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 6 [6400/22581 (28%)]\tLoss: 0.109139\tSteps: 1785200\n",
      "Train Epoch: 6 [12800/22581 (57%)]\tLoss: 0.105261\tSteps: 1805200\n",
      "Train Epoch: 6 [19200/22581 (85%)]\tLoss: 0.112667\tSteps: 1825200\n",
      "\n",
      "Test set: Average loss: 0.0982, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 7 [6400/22581 (28%)]\tLoss: 0.114409\tSteps: 1855800\n",
      "Train Epoch: 7 [12800/22581 (57%)]\tLoss: 0.108866\tSteps: 1875800\n",
      "Train Epoch: 7 [19200/22581 (85%)]\tLoss: 0.105607\tSteps: 1895800\n",
      "\n",
      "Test set: Average loss: 0.0981, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 8 [6400/22581 (28%)]\tLoss: 0.101280\tSteps: 1926400\n",
      "Train Epoch: 8 [12800/22581 (57%)]\tLoss: 0.105669\tSteps: 1946400\n",
      "Train Epoch: 8 [19200/22581 (85%)]\tLoss: 0.112856\tSteps: 1966400\n",
      "\n",
      "Test set: Average loss: 0.0980, Accuracy: 6230/6452 (97%)\n",
      "\n",
      "Train Epoch: 9 [6400/22581 (28%)]\tLoss: 0.107726\tSteps: 1997000\n",
      "Train Epoch: 9 [12800/22581 (57%)]\tLoss: 0.114520\tSteps: 2017000\n",
      "Train Epoch: 9 [19200/22581 (85%)]\tLoss: 0.110635\tSteps: 2037000\n",
      "\n",
      "Test set: Average loss: 0.0980, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 10 [6400/22581 (28%)]\tLoss: 0.111548\tSteps: 2067600\n",
      "Train Epoch: 10 [12800/22581 (57%)]\tLoss: 0.113187\tSteps: 2087600\n",
      "Train Epoch: 10 [19200/22581 (85%)]\tLoss: 0.107914\tSteps: 2107600\n",
      "\n",
      "Test set: Average loss: 0.0979, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 1 [6400/22581 (28%)]\tLoss: 0.116802\tSteps: 2138200\n",
      "Train Epoch: 1 [12800/22581 (57%)]\tLoss: 0.098957\tSteps: 2158200\n",
      "Train Epoch: 1 [19200/22581 (85%)]\tLoss: 0.113264\tSteps: 2178200\n",
      "\n",
      "Test set: Average loss: 0.0979, Accuracy: 6229/6452 (97%)\n",
      "\n",
      "Train Epoch: 2 [6400/22581 (28%)]\tLoss: 0.103303\tSteps: 2208800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-aa5f7cd762db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataGeneratorTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataGeneratorTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ae4f5244c5b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(ep, dataGeneratorTrain)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# negative log likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#loss = loss1(output, target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torchEnv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torchEnv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# splitting the whole data into train and test data\n",
    "totDf = pd.read_csv('KDDTrain+.csv', header = None)\n",
    "trainDf, testDf = train_test_split(totDf, test_size = 0.2, random_state = 2)\n",
    "\n",
    "# generating test dataset\n",
    "xTest, yTest = dataPreprocessing(testDf)\n",
    "DatasetTest = datasetTorch(xTest, yTest)\n",
    "DataGeneratorTest = DataLoader(DatasetTest, batch_size = len(DatasetTest), shuffle = True)\n",
    "\n",
    "# add model TCN or TCAN\n",
    "\n",
    "# generating folds for cross validation\n",
    "xTrain, yTrain = dataPreprocessing(trainDf)\n",
    "kf= KFold(n_splits=8)\n",
    "kf.get_n_splits(xTrain) # splitting up the traindata\n",
    "fold = 0\n",
    "for trainIndex, valIndex in kf.split(xTrain):\n",
    "    #print(\"Train: \", trainIndex, \"TEST: \", valIndex)\n",
    "    fold += 1\n",
    "    print(\"Now running fold {}.\".format(fold))\n",
    "    X_train, X_val = xTrain[trainIndex], xTrain[valIndex]\n",
    "    Y_train, Y_val = yTrain[trainIndex], yTrain[valIndex]\n",
    "    # generating train dataset\n",
    "    DatasetTrain = datasetTorch(X_train, Y_train)\n",
    "    DataGeneratorTrain = DataLoader(DatasetTrain, batch_size = 64, shuffle = True)\n",
    "    # generating validation dataset\n",
    "    DatasetVal = datasetTorch(X_val, Y_val)\n",
    "    DataGeneratorVal = DataLoader(DatasetVal, batch_size = len(DatasetVal), shuffle = True)\n",
    "\n",
    "    # train function on DataGeneratorTrain\n",
    "    # test function on DataGenertatorVal and average the accuracy for all the validation folds\n",
    "    \n",
    "    # make code below a function\n",
    "    if __name__ == \"__main__\":\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train(epoch, DataGeneratorTrain)\n",
    "            test(DataGeneratorTest)\n",
    "            if epoch % 10 == 0:\n",
    "                lr /= 10\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "    \n",
    "# get_metric function on DataGeneratorTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      3398\n",
      "           1       0.97      0.96      0.96      3054\n",
      "\n",
      "    accuracy                           0.97      6452\n",
      "   macro avg       0.97      0.96      0.97      6452\n",
      "weighted avg       0.97      0.97      0.97      6452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(DataGeneratorTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNameTest = 'KDDTest+.csv'\n",
    "testDf = pd.read_csv(fileNameTest, header = None)\n",
    "xTest, yTest = dataPreprocessing(testDf)\n",
    "DatasetTest = datasetTorch(xTest, yTest)\n",
    "DataGeneratorTest = DataLoader(DatasetTest, batch_size = len(DatasetTest), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77      3594\n",
      "           1       0.91      0.67      0.77      4902\n",
      "\n",
      "    accuracy                           0.77      8496\n",
      "   macro avg       0.79      0.79      0.77      8496\n",
      "weighted avg       0.81      0.77      0.77      8496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(DataGeneratorTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
